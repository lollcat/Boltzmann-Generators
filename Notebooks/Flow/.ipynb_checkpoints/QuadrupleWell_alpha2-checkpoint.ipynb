{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df8ade93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "cwd_path = Path.cwd(); set_path = str(cwd_path.parent.parent); os.chdir(set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6fa94bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from TargetDistributions.DoubleWell import QuadrupleWellEnergy\n",
    "import torch\n",
    "from FittedModels.utils.plotting_utils import plot_sampling_info, plot_divergences\n",
    "torch.manual_seed(5)\n",
    "from ImportanceSampling.VanillaImportanceSampler import VanillaImportanceSampling\n",
    "from FittedModels.train import LearntDistributionManager\n",
    "from Utils.numerical_utils import quadratic_function as expectation_function\n",
    "from FittedModels.Models.FlowModel import FlowModel\n",
    "from FittedModels.utils.plotting_utils import plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "from FittedModels.utils.plotting_utils import plot_samples_vs_contours_quadruple_well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "181e6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotter(*args, **kwargs):\n",
    "    # wrap plotting function like this so it displays during training\n",
    "    plot_samples_vs_contours_quadruple_well(*args, **kwargs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d975008",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = QuadrupleWellEnergy(a=-0.5, b=-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64b033a",
   "metadata": {},
   "source": [
    "# alpha=2 divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6112061",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "torch.set_default_dtype(torch.float64)\n",
    "# ******************* Parameters *******************\n",
    "dim = 4\n",
    "epochs = int(1e4)\n",
    "n_samples_estimation = int(1e5)\n",
    "batch_size = int(512)\n",
    "lr = 5e-5\n",
    "train_prior = False\n",
    "weight_decay = 1e-6\n",
    "clip_grad_norm = False\n",
    "optimizer = \"Adam\"\n",
    "flow_type = \"RealNVP\"\n",
    "loss_type = \"DReG\"\n",
    "initial_flow_scaling = 1.5\n",
    "n_flow_steps = 64\n",
    "annealing = True\n",
    "n_plots = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da729286",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)  # 0\n",
    "learnt_sampler = FlowModel(x_dim=dim, n_flow_steps=n_flow_steps,\n",
    "                           scaling_factor=initial_flow_scaling, flow_type=flow_type)\n",
    "tester = LearntDistributionManager(target, learnt_sampler, VanillaImportanceSampling, loss_type=loss_type,\n",
    "                                   lr=lr, optimizer=optimizer, annealing=annealing, weight_decay=weight_decay)\n",
    "\n",
    "plotter(tester)\n",
    "plt.show()\n",
    "expectation_before, info_before = tester.estimate_expectation(n_samples_estimation, expectation_function)\n",
    "\n",
    "if train_prior:\n",
    "    history_prior = tester.train_prior(epochs=200, batch_size=batch_size, lr=5e-3)\n",
    "    plot_history(history_prior)\n",
    "    plt.show()\n",
    "    plotter(tester)\n",
    "    plt.show()\n",
    "    expectation_prior_trained, info_prior = tester.estimate_expectation(n_samples_estimation, expectation_function)\n",
    "\n",
    "\n",
    "history = tester.train(epochs, batch_size=batch_size, clip_grad_norm=clip_grad_norm, max_grad_norm=1,\n",
    "                       intermediate_plots=True, plotting_func=plotter, n_plots=n_plots)\n",
    "plot_history(history)\n",
    "plt.show()\n",
    "plot_divergences(history)\n",
    "plt.show()\n",
    "plot_sampling_info(history)\n",
    "plt.show()\n",
    "\n",
    "expectation, info = tester.estimate_expectation(n_samples_estimation, expectation_function)\n",
    "\n",
    "print(f\"estimate before training is {expectation_before} \\n\"\n",
    "      f\"estimate after training is {expectation} \\n\"\n",
    "      f\"effective sample size before is {info_before['effective_sample_size'] / n_samples_estimation}\\n\"\n",
    "      f\"effective sample size after train is {info['effective_sample_size'] / n_samples_estimation}\\n\"\n",
    "      f\"variance in weights is {torch.var(info['normalised_sampling_weights'])}\")\n",
    "\n",
    "if train_prior:\n",
    "    print(f\"estimate after prior training is {expectation_prior_trained} \\n\"\n",
    "        f\"effective sample size trained prior is {info_prior['effective_sample_size'] / n_samples_estimation}\\n\")\n",
    "\n",
    "plotter(tester, n_samples=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f05bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
