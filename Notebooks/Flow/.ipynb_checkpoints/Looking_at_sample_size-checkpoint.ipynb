{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9fbda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "cwd_path = Path.cwd(); set_path = str(cwd_path.parent.parent); os.chdir(set_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35219e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from FittedModels.utils import plot_distributions, plot_samples, plot_sampling_info, plot_divergences\n",
    "torch.manual_seed(5)\n",
    "from ImportanceSampling.VanillaImportanceSampler import VanillaImportanceSampling\n",
    "from FittedModels.utils import plot_distributions\n",
    "from FittedModels.train import LearntDistributionManager\n",
    "from Utils import plot_func2D, MC_estimate_true_expectation, plot_distribution, expectation_function\n",
    "from FittedModels.Models.FlowModel import FlowModel\n",
    "from FittedModels.utils import plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "from TargetDistributions.MoG import MoG\n",
    "from TargetDistributions.MoG import custom_MoG\n",
    "import numpy as np\n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32ffd25",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122d9163",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## simple version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c260f921",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_points_space = torch.logspace(torch.log10(torch.tensor(1)), torch.log10(torch.tensor(max_n_samples)), \n",
    "                                    range_n_points, dtype=torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c7c5ad",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def estimate_key_info(tester, n_iter=20, max_n_samples=int(1e4), range_n_points=10):\n",
    "    n_points_space = torch.logspace(torch.log10(torch.tensor(1)), torch.log10(torch.tensor(max_n_samples)), \n",
    "                                    range_n_points, dtype=torch.int)\n",
    "    kl_hist = []\n",
    "    alpha_2_hist = []\n",
    "    for n_samples in tqdm(n_points_space):\n",
    "        kl_list = []\n",
    "        alpha_2_list = []\n",
    "        for i in range(n_iter):\n",
    "            batch_size = int(n_samples)\n",
    "            n_samples = int(n_samples)\n",
    "            x_samples, log_q_x = tester.learnt_sampling_dist(batch_size)\n",
    "            log_p_x = tester.target_dist.log_prob(x_samples)\n",
    "            #DReG_alpha_2_loss = tester.dreg_alpha_divergence_loss(x_samples, log_q_x, log_p_x))\n",
    "            #kl_loss = KL_loss(x_samples_not_used, log_q_x, log_p_x)\n",
    "            kl_MC_estimate = tester.kl_MC_estimate(n_samples)\n",
    "            alpha_2_MC_estimate = tester.alpha_divergence_MC_estimate(n_samples)\n",
    "            kl_list.append(kl_MC_estimate)\n",
    "            alpha_2_list.append(alpha_2_MC_estimate)\n",
    "        kl_hist.append(kl_list)\n",
    "        alpha_2_hist.append(alpha_2_list)\n",
    "    return np.array(kl_hist), np.array(alpha_2_hist), n_points_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80333252",
   "metadata": {},
   "source": [
    "## More complicated function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4387dd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_MC_estimator(log_q_x, log_p_x):\n",
    "    kl = log_q_x - log_p_x\n",
    "    kl_loss = torch.mean(kl)\n",
    "    return kl_loss.item()\n",
    "\n",
    "\n",
    "def alpha_divergence_MC_estimator(log_q_x, log_p_x, alpha=2):\n",
    "    alpha_one_minus_alpha_sign = torch.sign(torch.tensor(alpha * (1 - alpha)))\n",
    "    N = torch.tensor(log_p_x.shape[0])\n",
    "    log_alpha_divergence = -alpha_one_minus_alpha_sign * \\\n",
    "                           (torch.logsumexp(alpha*(log_p_x - log_q_x), dim=-1) - torch.log(N))\n",
    "    return log_alpha_divergence.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a970207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(target, n_points, batch_size=int(1e5)):\n",
    "    batch_size = min(n_points, batch_size)\n",
    "    n_batches = n_points/batch_size\n",
    "    assert n_batches % 1 == 0\n",
    "    log_p_x_list = []\n",
    "    log_q_x_list = []\n",
    "    for i in range(int(n_batches)):\n",
    "        with torch.no_grad():\n",
    "            x_samples, log_q_x = tester.learnt_sampling_dist(batch_size)\n",
    "            log_p_x = tester.target_dist.log_prob(x_samples)\n",
    "        log_p_x = log_p_x.detach()\n",
    "        log_q_x = log_q_x.detach()\n",
    "        log_p_x_list.append(log_p_x)\n",
    "        log_q_x_list.append(log_q_x)\n",
    "    return torch.cat(log_q_x_list), torch.cat(log_p_x_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2517515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "def estimate_key_info(tester, max_n_samples=1e6, min_n_samples=10, n_runs_max=10):\n",
    "    kl_hist = []\n",
    "    alpha_2_hist = []\n",
    "    n_points_space = torch.logspace(torch.log10(torch.tensor(min_n_samples)), torch.log10(torch.tensor(max_n_samples)), \n",
    "                                    int(np.log10(max_n_samples) - np.log10(min_n_samples)+1), dtype=torch.int)\n",
    "    batch_size = int(max_n_samples*n_runs_max)\n",
    "    log_q_x, log_p_x = get_probs(target, batch_size)\n",
    "    for n_samples in tqdm(n_points_space):\n",
    "        kl_list = []\n",
    "        alpha_2_list = []\n",
    "        n_iter = batch_size / n_samples\n",
    "        assert n_iter % 1 == 0\n",
    "        n_iter = int(n_iter)\n",
    "        for i in range(n_iter):\n",
    "            n_samples = int(n_samples)\n",
    "            log_q_x_slice = log_q_x[n_samples*i: n_samples*(i+1)]\n",
    "            log_p_x_slice = log_p_x[n_samples*i: n_samples*(i+1)]\n",
    "            kl_MC_estimate = kl_MC_estimator(log_q_x_slice, log_p_x_slice)\n",
    "            alpha_2_MC_estimate = alpha_divergence_MC_estimator(log_q_x_slice, log_p_x_slice)\n",
    "            kl_list.append(kl_MC_estimate)\n",
    "            alpha_2_list.append(alpha_2_MC_estimate)\n",
    "        kl_hist.append(kl_list)\n",
    "        alpha_2_hist.append(alpha_2_list)\n",
    "    return kl_hist, alpha_2_hist, n_points_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vars(hist_list):\n",
    "    return [np.var(run_list) for run_list in hist_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f792421",
   "metadata": {},
   "source": [
    "# Easy problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # 0 breaks it within 1000 epochs\n",
    "dim = 2\n",
    "n_samples_estimation = int(1e4)\n",
    "target = MoG(dim=dim, n_mixes=10, min_cov=0, loc_scaling=3)\n",
    "true_expectation = MC_estimate_true_expectation(target, expectation_function, int(1e6))\n",
    "torch.manual_seed(1)\n",
    "learnt_sampler = FlowModel(x_dim=dim, n_flow_steps=3, scaling_factor=5.0) #, flow_type=\"RealNVP\")\n",
    "tester = LearntDistributionManager(target, learnt_sampler, VanillaImportanceSampling, loss_type=\"DReG\", lr=1e-3)\n",
    "expectation_before, info_before = tester.estimate_expectation(n_samples_estimation, expectation_function)\n",
    "samples_fig_before = plot_samples(tester) # this just looks at 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a3cf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_hist, alpha_2_hist, n_points_space = estimate_key_info(tester, int(1e3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd90bef2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#kl_hist, alpha_2_hist, n_points_space = estimate_key_info(tester, n_iter=10, max_n_samples=int(1e3), \n",
    "#                                                          min_n_samples=10, range_n_points=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543d4bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(n_points_space, get_vars(kl_hist))\n",
    "plt.title(\"kl estimate variance\")\n",
    "plt.figure()\n",
    "plt.plot(n_points_space, get_vars(alpha_2_hist))\n",
    "plt.title(\"alpha_2 divergence estimate variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b7ef04",
   "metadata": {},
   "source": [
    "# Hard Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5a8015",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0) # 0 breaks it within 1000 epochs\n",
    "dim = 3\n",
    "n_samples_estimation = int(1e4)\n",
    "target = MoG(dim=dim, n_mixes=10, min_cov=0, loc_scaling=3)\n",
    "true_expectation = MC_estimate_true_expectation(target, expectation_function, int(1e6))\n",
    "torch.manual_seed(1)\n",
    "learnt_sampler = FlowModel(x_dim=dim, n_flow_steps=3, scaling_factor=5.0) #, flow_type=\"RealNVP\")\n",
    "tester = LearntDistributionManager(target, learnt_sampler, VanillaImportanceSampling, loss_type=\"DReG\", lr=1e-3)\n",
    "expectation_before, info_before = tester.estimate_expectation(n_samples_estimation, expectation_function)\n",
    "samples_fig_before = plot_samples(tester) # this just looks at 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2450213e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kl_hist, alpha_2_hist, n_points_space = estimate_key_info(tester, int(1e6), min_n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039b524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(n_points_space, get_vars(kl_hist))\n",
    "plt.title(\"kl estimate variance\")\n",
    "plt.figure()\n",
    "plt.plot(n_points_space, get_vars(alpha_2_hist))\n",
    "plt.title(\"alpha_2 divergence estimate variance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566f6fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
